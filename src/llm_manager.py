"""
LLMç®¡ç†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« - qwen2:7bå¯¾å¿œOllamaé€£æº

Google Cloud E2ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆ8GB RAMï¼‰æœ€é©åŒ–
ãƒãƒ«ãƒãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆå¯¾å¿œã®è»½é‡LLMæ¨è«–æ©Ÿèƒ½
"""

import json
import logging
import asyncio
from typing import Optional, Dict, Any, List
import requests
from datetime import datetime

from config.settings import Settings

# LangSmithç”¨
try:
    from langsmith import traceable
    LANGSMITH_AVAILABLE = True
except ImportError:
    LANGSMITH_AVAILABLE = False
    # ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ã®ãƒ€ãƒŸãƒ¼å®Ÿè£…
    def traceable(name=None):
        def decorator(func):
            return func
        return decorator

logger = logging.getLogger(__name__)
settings = Settings()

class LLMManager:
    """LLMç®¡ç†ã‚¯ãƒ©ã‚¹ - qwen2:7bç‰¹åŒ–"""
    
    def __init__(self):
        self.model_name = settings.llm_model_name
        self.ollama_host = settings.ollama_host
        self.base_url = f"http://{self.ollama_host}"
        self.timeout = settings.ollama_timeout
        
        # qwen2:7bæœ€é©åŒ–è¨­å®š
        self.generation_config = {
            "temperature": 0.7,
            "top_p": 0.9,
            "top_k": 40,
            "max_tokens": 2048,
            "stop": ["Human:", "Assistant:", "\n\n---"],
            "stream": False
        }
        
        logger.info(f"LLM ManageråˆæœŸåŒ–: {self.model_name} @ {self.base_url}")
    
    def check_model_availability(self) -> bool:
        """qwen2:7bãƒ¢ãƒ‡ãƒ«ã®åˆ©ç”¨å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯"""
        try:
            response = requests.get(
                f"{self.base_url}/api/tags",
                timeout=10
            )
            
            if response.status_code == 200:
                models = response.json().get("models", [])
                available_models = [model["name"] for model in models]
                
                is_available = any(
                    self.model_name in model for model in available_models
                )
                
                if is_available:
                    logger.info(f"âœ… {self.model_name} ãƒ¢ãƒ‡ãƒ«åˆ©ç”¨å¯èƒ½")
                else:
                    logger.warning(f"âš ï¸  {self.model_name} ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    logger.info(f"åˆ©ç”¨å¯èƒ½ãƒ¢ãƒ‡ãƒ«: {available_models}")
                
                return is_available
                
        except Exception as e:
            logger.error(f"âŒ Ollamaæ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    @traceable(name="llm_generate_response")
    def generate_response(
        self, 
        prompt: str, 
        context: Optional[str] = None,
        conversation_history: Optional[List[str]] = None,
        **kwargs
    ) -> str:
        """qwen2:7bã‚’ä½¿ç”¨ã—ãŸå¿œç­”ç”Ÿæˆï¼ˆLangSmithãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°å¯¾å¿œï¼‰"""
        
        # LangSmithãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ç”¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        trace_metadata = {
            "model": self.model_name,
            "has_context": bool(context),
            "has_history": bool(conversation_history),
            "prompt_length": len(prompt)
        }
        
        if LANGSMITH_AVAILABLE and settings.enable_langsmith:
            logger.info(f"LangSmithãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°æœ‰åŠ¹ - ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: {settings.langsmith_project}")
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä»˜ããƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
        full_prompt = self._build_prompt(prompt, context, conversation_history)
        
        # ç”Ÿæˆè¨­å®šã®ãƒãƒ¼ã‚¸
        generation_params = {**self.generation_config, **kwargs}
        
        try:
            # Ollama APIå‘¼ã³å‡ºã—
            response = requests.post(
                f"{self.base_url}/api/generate",
                json={
                    "model": self.model_name,
                    "prompt": full_prompt,
                    "options": {
                        "temperature": generation_params["temperature"],
                        "top_p": generation_params["top_p"],
                        "top_k": generation_params["top_k"],
                        "num_predict": generation_params["max_tokens"],
                        "stop": generation_params["stop"]
                    },
                    "stream": generation_params["stream"]
                },
                timeout=self.timeout
            )
            
            if response.status_code == 200:
                result = response.json()
                generated_text = result.get("response", "").strip()
                
                # ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ç”¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ›´æ–°
                trace_metadata.update({
                    "response_length": len(generated_text),
                    "eval_count": result.get("eval_count", 0),
                    "eval_duration_ms": result.get("eval_duration", 0) / 1e6
                })
                
                # ç”Ÿæˆçµ±è¨ˆãƒ­ã‚°
                if "eval_count" in result:
                    logger.info(
                        f"ç”Ÿæˆå®Œäº†: {result.get('eval_count', 0)}ãƒˆãƒ¼ã‚¯ãƒ³, "
                        f"{result.get('eval_duration', 0)/1e6:.1f}ms"
                    )
                
                return generated_text
            else:
                logger.error(f"LLMç”Ÿæˆã‚¨ãƒ©ãƒ¼: {response.status_code} - {response.text}")
                return "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€å¿œç­”ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"
                
        except requests.Timeout:
            logger.error("LLMç”Ÿæˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
            return "å‡¦ç†æ™‚é–“ãŒé•·ã™ãã‚‹ãŸã‚ã€ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸã€‚ã‚ˆã‚Šç°¡æ½”ãªè³ªå•ã§ãŠè©¦ã—ãã ã•ã„ã€‚"
        except Exception as e:
            logger.error(f"LLMç”Ÿæˆä¾‹å¤–: {e}")
            return "ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã—ã°ã‚‰ãå¾Œã§ãŠè©¦ã—ãã ã•ã„ã€‚"
    
    async def generate_response_async(
        self, 
        prompt: str, 
        context: Optional[str] = None,
        **kwargs
    ) -> str:
        """éåŒæœŸç‰ˆå¿œç­”ç”Ÿæˆ"""
        import aiohttp
        
        full_prompt = self._build_prompt(prompt, context)
        generation_params = {**self.generation_config, **kwargs}
        
        try:
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(self.timeout)) as session:
                async with session.post(
                    f"{self.base_url}/api/generate",
                    json={
                        "model": self.model_name,
                        "prompt": full_prompt,
                        "options": {
                            "temperature": generation_params["temperature"],
                            "top_p": generation_params["top_p"],
                            "top_k": generation_params["top_k"],
                            "num_predict": generation_params["max_tokens"],
                            "stop": generation_params["stop"]
                        },
                        "stream": False
                    }
                ) as response:
                    if response.status == 200:
                        result = await response.json()
                        return result.get("response", "").strip()
                    else:
                        logger.error(f"éåŒæœŸLLMç”Ÿæˆã‚¨ãƒ©ãƒ¼: {response.status}")
                        return "å¿œç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"
                        
        except asyncio.TimeoutError:
            logger.error("éåŒæœŸLLMç”Ÿæˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
            return "å‡¦ç†æ™‚é–“ãŒé•·ã™ãã‚‹ãŸã‚ã€ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸã€‚"
        except Exception as e:
            logger.error(f"éåŒæœŸLLMç”Ÿæˆä¾‹å¤–: {e}")
            return "ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"
    
    def health_check(self) -> bool:
        """Ollamaã‚µãƒ¼ãƒ“ã‚¹ã®ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
        try:
            response = requests.get(
                f"{self.base_url}/api/tags",
                timeout=5
            )
            if response.status_code == 200:
                # ãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨å¯èƒ½ã‹ã‚‚ç¢ºèª
                tags = response.json()
                available_models = [model["name"] for model in tags.get("models", [])]
                is_model_available = any(self.model_name in model for model in available_models)
                
                if is_model_available:
                    logger.info(f"LLMãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯æˆåŠŸ: {self.model_name}")
                    return True
                else:
                    logger.warning(f"LLMãƒ¢ãƒ‡ãƒ«æœªåˆ©ç”¨å¯èƒ½: {self.model_name}")
                    return False
            else:
                logger.warning(f"Ollamaã‚µãƒ¼ãƒ“ã‚¹å¿œç­”ã‚¨ãƒ©ãƒ¼: {response.status_code}")
                return False
                
        except Exception as e:
            logger.error(f"LLMãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯å¤±æ•—: {e}")
            return False
    
    def _build_prompt(self, question: str, context: Optional[str] = None, conversation_history: Optional[List[str]] = None) -> str:
        """RAGå¯¾å¿œãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰ï¼ˆä¼šè©±å±¥æ­´å¯¾å¿œï¼‰"""
        
        # ä¼šè©±å±¥æ­´ã®æ§‹ç¯‰
        history_text = ""
        if conversation_history:
            # æœ€æ–°10å›ã®ä¼šè©±ã®ã¿ä½¿ç”¨
            recent_history = conversation_history[-10:]
            history_text = "\n".join(recent_history)
            history_text = f"\n\néå»ã®ä¼šè©±:\n{history_text}\n"
        
        if context:
            # RAGã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä»˜ããƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            prompt = f"""ã‚ãªãŸã¯è¦ªåˆ‡ã§çŸ¥è­˜è±Šå¯ŒãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚æä¾›ã•ã‚ŒãŸæ–‡æ›¸ã®å†…å®¹ã«åŸºã¥ã„ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«æ­£ç¢ºã«ç­”ãˆã¦ãã ã•ã„ã€‚{history_text}

å‚è€ƒæ–‡æ›¸:
{context}

è³ªå•: {question}

å›ç­”ã¯ä»¥ä¸‹ã®ç‚¹ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„ï¼š
1. æä¾›ã•ã‚ŒãŸæ–‡æ›¸ã®å†…å®¹ã«åŸºã¥ã„ã¦å›ç­”ã™ã‚‹
2. æ–‡æ›¸ã«è¨˜è¼‰ã•ã‚Œã¦ã„ãªã„å†…å®¹ã«ã¤ã„ã¦ã¯æ¨æ¸¬ã›ãšã€ã€Œæ–‡æ›¸ã«ã¯è¨˜è¼‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€ã¨æ˜ç¤ºã™ã‚‹
3. å›ç­”ã¯å…·ä½“çš„ã§å®Ÿç”¨çš„ãªã‚‚ã®ã«ã™ã‚‹
4. æ—¥æœ¬èªã§è‡ªç„¶ãªæ–‡ç« ã§å›ç­”ã™ã‚‹

å›ç­”:"""
        else:
            # ä¸€èˆ¬çš„ãªè³ªå•ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            prompt = f"""ã‚ãªãŸã¯è¦ªåˆ‡ã§çŸ¥è­˜è±Šå¯ŒãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«é©åˆ‡ã«ç­”ãˆã¦ãã ã•ã„ã€‚{history_text}

è³ªå•: {question}

å›ç­”:"""
        
        return prompt
    
    def get_model_info(self) -> Dict[str, Any]:
        """ãƒ¢ãƒ‡ãƒ«æƒ…å ±å–å¾—"""
        try:
            response = requests.post(
                f"{self.base_url}/api/show",
                json={"name": self.model_name},
                timeout=10
            )
            
            if response.status_code == 200:
                return response.json()
            else:
                return {"error": f"ãƒ¢ãƒ‡ãƒ«æƒ…å ±å–å¾—å¤±æ•—: {response.status_code}"}
                
        except Exception as e:
            return {"error": f"ãƒ¢ãƒ‡ãƒ«æƒ…å ±å–å¾—ä¾‹å¤–: {e}"}
    
    def get_server_status(self) -> Dict[str, Any]:
        """Ollamaã‚µãƒ¼ãƒãƒ¼çŠ¶æ…‹å–å¾—"""
        try:
            # ã‚µãƒ¼ãƒãƒ¼ç”Ÿå­˜ç¢ºèª
            health_response = requests.get(
                f"{self.base_url}/api/tags",
                timeout=5
            )
            
            # å®Ÿè¡Œä¸­ãƒ¢ãƒ‡ãƒ«ç¢ºèª
            ps_response = requests.get(
                f"{self.base_url}/api/ps",
                timeout=5
            )
            
            status = {
                "server_alive": health_response.status_code == 200,
                "timestamp": datetime.now().isoformat(),
                "configured_model": self.model_name,
                "ollama_host": self.ollama_host
            }
            
            if ps_response.status_code == 200:
                running_models = ps_response.json().get("models", [])
                status["running_models"] = [
                    {
                        "name": model.get("name", ""),
                        "size": model.get("size", 0),
                        "digest": model.get("digest", "")[:12]
                    }
                    for model in running_models
                ]
            
            return status
            
        except Exception as e:
            return {
                "server_alive": False,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
llm_manager = LLMManager()

def get_llm_manager() -> LLMManager:
    """LLMç®¡ç†ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å–å¾—"""
    return llm_manager

# ä¾¿åˆ©é–¢æ•°
def generate_rag_response(question: str, context: str) -> str:
    """RAGå¿œç­”ç”Ÿæˆ"""
    return llm_manager.generate_response(question, context)

def check_llm_health() -> bool:
    """LLMãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    return llm_manager.check_model_availability()

if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    manager = LLMManager()
    
    print("ğŸ” ãƒ¢ãƒ‡ãƒ«åˆ©ç”¨å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯...")
    if manager.check_model_availability():
        print("âœ… ãƒ¢ãƒ‡ãƒ«åˆ©ç”¨å¯èƒ½")
        
        print("\nğŸ’¬ ãƒ†ã‚¹ãƒˆå¿œç­”ç”Ÿæˆ...")
        test_response = manager.generate_response("ã“ã‚“ã«ã¡ã¯ï¼è‡ªå·±ç´¹ä»‹ã‚’ã—ã¦ãã ã•ã„ã€‚")
        print(f"å¿œç­”: {test_response}")
        
        print("\nğŸ“Š ã‚µãƒ¼ãƒãƒ¼çŠ¶æ…‹...")
        status = manager.get_server_status()
        print(json.dumps(status, ensure_ascii=False, indent=2))
    else:
        print("âŒ ãƒ¢ãƒ‡ãƒ«åˆ©ç”¨ä¸å¯")